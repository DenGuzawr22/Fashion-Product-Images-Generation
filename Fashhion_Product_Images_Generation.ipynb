{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTr5u3gK6xPm"
      },
      "source": [
        "# Fashion Product Image Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvV8Mffk6xPn"
      },
      "source": [
        "This project will introduce a deep learning generative model capable of creating new fashion product images.\n",
        "\n",
        "The primary goal of this project is to produce an image that matches a given article type and color.\n",
        "\n",
        "The primary tools for this project are the [TensorFlow](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and the [Keras](https://keras.io/) open-source library for developing neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVMdqe3n6xPn"
      },
      "source": [
        "## Useful import\n",
        "Before use the script load to the colab folder `styles.csv` and `subset.zip`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JZNstt769wr",
        "outputId": "cfa15bf3-0b93-4658-9615-5aa65f5ba9bd"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset\n",
        "!mv styles.csv dataset/\n",
        "!mkdir dataset/images\n",
        "!unzip -qq subset.zip -d dataset/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPuw5wqU6xPo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from scipy import linalg\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hI4SqhS6xPo"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "Here are the utility functions defined for this project:\n",
        "\n",
        "- `get_dataset_folder_file_path`: Given a file name, it returns the path to that file.\n",
        "\n",
        "- `get_image_path`: Given an image ID and an optional folder path, it returns the path to the image file.\n",
        "\n",
        "- `labels_provider`: Given a list of one-hot encoded labels and a batch size, it returns a supplier that provides one batch of data from the list in each iteration.\n",
        "\n",
        "- `infinite_generator`: This is used by the label infinite lambda provider to create an infinite generator for a given value.\n",
        "\n",
        "- `label_provider`: A lambda function that generates an infinite sequence for a given value.\n",
        "\n",
        "You can execute this code to define and use these utility functions in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6vrBkag6xPp"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"dataset/\"\n",
        "\n",
        "IMG_FOLDER = DATASET_PATH + \"images/subset/\"\n",
        "\n",
        "def get_dataset_folder_file_path(file_name):\n",
        "    return DATASET_PATH + file_name\n",
        "\n",
        "def get_image_path(image_id, folder=IMG_FOLDER):\n",
        "    return folder + str(image_id) + \".jpg\"\n",
        "\n",
        "def labels_provider(l, n):\n",
        "   while len(l) > 0:\n",
        "      poped = l[:n]\n",
        "      l = l[n:]\n",
        "      yield poped\n",
        "\n",
        "def infinite_generator(value):\n",
        "    while True:\n",
        "        yield value\n",
        "\n",
        "label_provider = lambda a: infinite_generator(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2hqUGPP6xPp"
      },
      "source": [
        "### Ploters\n",
        "Here are the utility functions for image and chart plotting defined in the provided code:\n",
        "\n",
        "- `plot_images_by_id`: This function plots images given a list of image IDs. The number of IDs should be a multiple of 8.\n",
        "\n",
        "- `plot_random_images`: It plots random images from a given pandas DataFrame.\n",
        "\n",
        "- `plot_generated_images`: This function plots generated images from a list of image pixel data.\n",
        "\n",
        "- `plot_provided_images`: It plots images provided by an image provider.\n",
        "\n",
        "- `plot_model_input_and_output`: Used for showing input and output images of an autoencoder model. It plots the original images and the corresponding generated images.\n",
        "\n",
        "- `plot_same_model_input_and_output`: Similar to `plot_model_input_and_output`, but it can take the same batch of images as input.\n",
        "\n",
        "You can execute this code to define and use these utility functions for image and chart plotting in your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Sia2laE6xPp"
      },
      "outputs": [],
      "source": [
        "def plot_images_by_id(ids, folder=IMG_FOLDER):\n",
        "    rows = int(len(ids)/8) # should be multiple of 8\n",
        "    fig, axes = plt.subplots(rows, 8, figsize=(8, rows*3))\n",
        "\n",
        "    for i, ax in enumerate(axes.flatten()):\n",
        "        image_id = ids[i]\n",
        "        image_path = get_image_path(image_id, folder)\n",
        "        img = Image.open(image_path)\n",
        "        ax.imshow(img, cmap='Greys_r')\n",
        "        ax.set_title(image_id)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_random_images(df, num=8, path=IMG_FOLDER):\n",
        "  ids = df['id']\n",
        "  selected_image_ids = random.sample(ids.tolist(), num)\n",
        "  plot_images_by_id(selected_image_ids, path)\n",
        "\n",
        "\n",
        "def plot_generated_images(generated_images, nrows, ncols, no_space_between_plots=False, figsize=(15, 15)):\n",
        "  _, axs = plt.subplots(nrows, ncols, figsize=figsize,squeeze=False)\n",
        "\n",
        "  for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "      axs[i,j].axis('off')\n",
        "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
        "\n",
        "  if no_space_between_plots:\n",
        "    plt.subplots_adjust(wspace=0,hspace=0)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_provided_images(provider):\n",
        "    it = next(provider)\n",
        "    if(type(it) is tuple):\n",
        "       images, _ = it\n",
        "    else:\n",
        "        images = it\n",
        "\n",
        "    print(\"An image shape: \", images[1].shape)\n",
        "    plot_generated_images([images], 1, 5)\n",
        "\n",
        "    print(\"Images shape (numImages, high, width, numColors):\")\n",
        "    print(images.shape)\n",
        "\n",
        "# Used for showing autoencoder input and its corresponding output\n",
        "def plot_model_input_and_output(generator, model, num=6):\n",
        "   # Trasform 5 random images from validation set\n",
        "   val_x, val_y = next(generator)\n",
        "   if (len(val_x) < num):\n",
        "      val_x, val_y = next(generator) # redo\n",
        "\n",
        "   # get first 5 dataset images\n",
        "   real_imgs = val_x[:num]\n",
        "   labels = val_y[:num]\n",
        "   plot_generated_images([real_imgs], 1, num)\n",
        "\n",
        "   generated_imgs = model.predict([real_imgs,labels], verbose=0)\n",
        "   plot_generated_images([generated_imgs], 1, num)\n",
        "\n",
        "def plot_same_model_input_and_output(val_x, val_y, model, num=6):\n",
        "   real_imgs = val_x[:num]\n",
        "   labels = val_y[:num]\n",
        "   plot_generated_images([real_imgs], 1, num)\n",
        "\n",
        "   generated_imgs = model.predict([real_imgs,labels], verbose=0)\n",
        "   plot_generated_images([generated_imgs], 1, num)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwE5cRl56xPq"
      },
      "source": [
        "#### History ploters\n",
        "Here are the utility functions for plotting model history and loss changes:\n",
        "\n",
        "- `plot_fid`: This function plots Frechet Inception Distance (FID) values by checkpoints. It takes a list of FID values as input and plots them over the checkpoints.\n",
        "\n",
        "- `plot_losses_from_array`: It plots training and validation losses over epochs. It takes lists of training and validation losses as input and displays them on the same chart.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2uyFMMl6xPq"
      },
      "outputs": [],
      "source": [
        "def plot_fid(fid_values):\n",
        "  checkpoints = range(1, len(fid_values) + 1)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(checkpoints, fid_values, marker='o', linestyle='-')\n",
        "  plt.title('FID Changes by Checkpoint')\n",
        "  plt.xlabel('Checkpoint')\n",
        "  plt.ylabel('FID Value')\n",
        "  plt.grid(True)\n",
        "\n",
        "\n",
        "def plot_losses_from_array(training_losses, validation_losses):\n",
        "  epochs = list(range(1, len(training_losses) + 1))\n",
        "  plt.plot(epochs, training_losses, label='Training Loss',  linestyle='-')\n",
        "  plt.plot(epochs, validation_losses, label='Validation Loss', linestyle='-')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training and Validation Loss Over Epochs')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb7GAojj6xPq"
      },
      "source": [
        "## Dataset\n",
        "The [**Fashion Product Images (Small)**](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small?select=styles.csv) dataset, containing 80x60 colorful images of fashion products. Original dataset have 44000 products\n",
        "\n",
        "There are 143 article types half of them have less than 50 articles. We choose 12 the most stable and homogeneous article types as our working target: *Watches*, *Handbags*, *Sunglasses*, *Belts*, *Backpacks*, *Sarees*, *Deodorant*, *Nail Polish*, *Ties*, *Sports Shoes*, *Flip Flops*, *Formal Shoes*. Those 12 classes contains circa 12000 of products.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "Of0Lr9Q96xPq",
        "outputId": "bef0b4a0-11ba-401d-eb85-43284f9433d2"
      },
      "outputs": [],
      "source": [
        "### Possible classes:\n",
        "\n",
        "# \"Watches\"\n",
        "# \"Handbags\"\n",
        "# \"Sunglasses\"\n",
        "# \"Belts\"\n",
        "# \"Backpacks\"\n",
        "# \"Sarees\"\n",
        "# \"Deodorant\"\n",
        "# \"Nail Polish\"\n",
        "# \"Ties\"\n",
        "# \"Sports Shoes\"\n",
        "# \"Flip Flops\"\n",
        "# \"Formal Shoes\"\n",
        "\n",
        "# Flexible list of the article types that will be elaborated by neural networks\n",
        "CLASSES = [ \"Sunglasses\", \"Flip Flops\", \"Nail Polish\", \"Sarees\", \"Deodorant\"]\n",
        "\n",
        "def filter_articles(df, classes):\n",
        "    return df[df['articleType'].isin(classes)]\n",
        "\n",
        "def get_dataframe_by(df, column_name, value):\n",
        "    return df[df[column_name] == value]\n",
        "\n",
        "def get_dataframe_by_article_type(df, article_type):\n",
        "    return get_dataframe_by(df, \"articleType\", article_type)\n",
        "\n",
        "def get_dataframe_by_color(df, color):\n",
        "    return get_dataframe_by(df, \"baseColour\", color)\n",
        "\n",
        "def load_df(classes):\n",
        "    df = pd.read_csv(get_dataset_folder_file_path('styles.csv'), dtype=str)\n",
        "    df = df[df.notnull()[\"baseColour\"]] # remove null values from basecolor column\n",
        "    df['articleType'].value_counts()\n",
        "    df = filter_articles(df, classes)\n",
        "    return df\n",
        "\n",
        "def image_exists(id):\n",
        "    image_filename = f'{IMG_FOLDER}{id}.jpg'\n",
        "    return os.path.exists(image_filename)\n",
        "\n",
        "\n",
        "df = load_df(CLASSES)\n",
        "df = df[df['id'].apply(image_exists)]\n",
        "\n",
        "print(df['articleType'].value_counts()) # number of products for each article type\n",
        "\n",
        "for t in df['articleType'].unique():\n",
        "    plot_random_images(get_dataframe_by_article_type(df, t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKWFYVb86xPq"
      },
      "source": [
        "### Color aggregation\n",
        "In the dataset the products have 42 different colors. To be able generate images of a specific color in a stable way we should aggregate similar colors. After aggregation our articles will have 10 colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "be3GD3Fg6xPr",
        "outputId": "a93b03f6-43a8-44bd-ba74-3d22d6c5e5f7"
      },
      "outputs": [],
      "source": [
        "baseColour = \"baseColour\"\n",
        "\n",
        "def aggregate_df_colors(df):\n",
        "    df[baseColour] = df[baseColour].replace(\"Lime Green\", \"Green\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Fluorescent Green\", \"Green\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Sea Green\", \"Green\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Teal\", \"Green\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Taupe\", \"Grey\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Grey Melange\", \"Grey\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Steel\", \"Grey\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Silver\", \"Grey\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Skin\", \"Beige\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Grey\", \"White\") # White + Grey\n",
        "    df[baseColour] = df[baseColour].replace(\"Beige\", \"White\") # White + Beige\n",
        "    df[baseColour] = df[baseColour].replace(\"Off White\", \"White\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Mushroom Brown\", \"Brown\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Nude\", \"Brown\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Coffee Brown\", \"Brown\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Burgundy\", \"Brown\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Copper\", \"Brown\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Bronze\", \"Brown\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Tan\", \"Brown\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Khaki\", \"Brown\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Rose\", \"Red\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Orange\", \"Red\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Rust\", \"Red\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Maroon\", \"Red\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Magenta\", \"Pink\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Peach\", \"Pink\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Mauve\", \"Purple\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Lavender\", \"Purple\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Metallic\", \"Black\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Charcoal\", \"Black\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Turquoise Blue\", \"Blue\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Mustard\", \"Yellow\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Gold\", \"Yellow\")\n",
        "\n",
        "    df[baseColour] = df[baseColour].replace(\"Cream\", \"Multi\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Olive\", \"Multi\")\n",
        "    df[baseColour] = df[baseColour].replace(\"Navy Blue\", \"Multi\")\n",
        "    return df\n",
        "\n",
        "def get_df():\n",
        "    df = load_df(CLASSES)\n",
        "    df = df[df['id'].apply(image_exists)]\n",
        "    print(\"before aggreation number of colors: \", len(df[baseColour].value_counts()))\n",
        "    df = aggregate_df_colors(df)\n",
        "    print(\"After aggreation number of colors: \", len(df[baseColour].value_counts()))\n",
        "    return df\n",
        "\n",
        "df = get_df()\n",
        "\n",
        "print(df[baseColour].value_counts()) # number of products for each color\n",
        "\n",
        "for color in df[baseColour].unique():\n",
        "    print(color)\n",
        "    plot_random_images(get_dataframe_by_color(df, color))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KDFIyUs6xPr"
      },
      "source": [
        "### Dataset loading and preprocessing\n",
        "The `MultiLabelImageDataGenerator` serves as an adapter for a [`DataFrameIterator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_dataframe), facilitating the standardization of outputs for both \"categorical\" and \"multi_label\" configurations.\n",
        "\n",
        "When working with a `DataFrameIterator`, the output format varies:\n",
        "- In a **`categorical`** single-label setup, the output is a tuple containing an array of images and another array of labels that are **one-hot encoded**:\n",
        "  - For example: `([img_1, img_2], [one_hot_1, one_hot_2])`\n",
        "\n",
        "- In a **`multi_label`** class mode setup, the output is a tuple with an array of images as the first element, while the second element is an array that comprises multiple arrays, each containing labels in **string format**. One-hot encoding must be applied separately in this case:\n",
        "  - For example: `([img_1, img_2], [[\"type_1\", \"type_2\"], [\"color_1\", \"color_2\"]])`\n",
        "\n",
        "The primary purpose of the `MultiLabelImageDataGenerator` is to abstract away the complexities of one-hot encoding and the concatenation of multiple one-hot encoded labels. This abstraction enables the use of the same neural network for varying numbers of labels. The result is a consistent output format for both \"categorical\" and \"multi_label\" configurations, ensuring seamless compatibility between different label setups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQRNB61I6xPr"
      },
      "outputs": [],
      "source": [
        "class MultiLabelImageDataGenerator:\n",
        "    def __init__(self, generator, articleType_encoder, color_encoder):\n",
        "        self.generator = generator\n",
        "        self.articleType_encoder = articleType_encoder\n",
        "        self.color_encoder = color_encoder\n",
        "        self.articl_n_classes =  len(articleType_encoder.classes_)\n",
        "        self.color_n_classes =  len(color_encoder.classes_)\n",
        "        self.num_classes = self.articl_n_classes + self.color_n_classes\n",
        "        self.class_indicies = articleType_encoder.classes_\n",
        "\n",
        "        all_artcle = to_categorical(articleType_encoder.transform(generator.labels[0]))\n",
        "        all_colors = to_categorical(color_encoder.transform(generator.labels[1]))\n",
        "\n",
        "        concatenated = []\n",
        "        for i in range(len(all_artcle)):\n",
        "            concatenated.append(all_artcle[i].tolist() + all_colors[i].tolist())\n",
        "\n",
        "        self.labels = np.array(concatenated, dtype=np.float32)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        x, y = next(self.generator)\n",
        "        articleType_one_hot = to_categorical(self.articleType_encoder.transform(y[0]), num_classes=self.articl_n_classes)\n",
        "        color_one_hot = to_categorical(self.color_encoder.transform(y[1]), num_classes=self.color_n_classes)\n",
        "        concatenated = []\n",
        "        for i in range(len(articleType_one_hot)):\n",
        "            concatenated.append(articleType_one_hot[i].tolist() + color_one_hot[i].tolist())\n",
        "\n",
        "        return x, np.array(concatenated, dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.generator)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO7oIG0l6xPr"
      },
      "source": [
        "#### Data provider definition\n",
        "The data provider is built upon the [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator), which uses a \"flow from dataframe\" approach for flexible image loading from a directory. Below is an improved description of the provided code:\n",
        "\n",
        "The `create_data_provider_df` function is responsible for creating a data provider for dataset images. It allows for customizable loading of image data and is designed to work with both \"categorical\" and \"multi_output\" label configurations. Here are the details:\n",
        "\n",
        "- **Args**:\n",
        "  - `data_dir` (string): The path to the directory containing the images.\n",
        "  - `class_mode` (str):\n",
        "    - \"categorical\" if one-hot encoded labels should only contain \"articleType\".\n",
        "    - \"multi_output\" if one-hot encoded labels should contain both \"articleType\" and \"baseColour\".\n",
        "  - `batch_size` (int, optional): The number of images to load at each iteration. Defaults to 64.\n",
        "  - `image_size` (tuple, optional): The size of the images. Defaults to (80, 60).\n",
        "  - `rgb` (bool, optional): Set to True if images are in color (RGB). The default is False.\n",
        "  - `tanh_rescale` (bool, optional): If set to True, image values are rescaled from -1 to 1. The default is False.\n",
        "  - `validation_split` (float, optional): Defines the portion of the data to be split into the validation dataset. The default is 0.1.\n",
        "\n",
        "- **Returns**:\n",
        "  - If `class_mode` is \"multi_output\" the function returns a `MultiLabelImageDataGenerator`.\n",
        "  - For other `class_mode` values, it returns a `DataFrameIterator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvKL_pBm6xPr"
      },
      "outputs": [],
      "source": [
        "def create_data_provider_df(\n",
        "        data_dir,\n",
        "        class_mode,\n",
        "        batch_size=64,\n",
        "        image_size = (80,60),\n",
        "        rgb=False,\n",
        "        tanh_rescale=False,\n",
        "        validation_split = 0.1):\n",
        "    color_mode =  \"rgb\" if (rgb) else \"grayscale\"\n",
        "    y = [\"articleType\", \"baseColour\"] if(class_mode==\"multi_output\") else \"articleType\"\n",
        "\n",
        "    df = get_df()\n",
        "    def append_ext(id): return id+\".jpg\"\n",
        "    df['id'] = df['id'].apply(append_ext)\n",
        "\n",
        "    articleType_encoder = LabelEncoder()\n",
        "    color_encoder = LabelEncoder()\n",
        "    articleType_encoder.fit(df[\"articleType\"].unique())\n",
        "    color_encoder.fit(df[\"baseColour\"].unique())\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        validation_split=validation_split,\n",
        "    )\n",
        "\n",
        "    if(tanh_rescale):\n",
        "        def prep_fn(img):\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "            img = (img - 0.5) * 2\n",
        "            return img\n",
        "        datagen.preprocessing_function = prep_fn\n",
        "    else:\n",
        "        datagen.rescale = 1.0 / 255.0,  # Scale pixel values between 0 and 1\n",
        "\n",
        "    train_data_provider = datagen.flow_from_dataframe(\n",
        "        df,\n",
        "        data_dir,\n",
        "        x_col=\"id\",\n",
        "        y_col=y,\n",
        "        color_mode=color_mode,\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=class_mode,#'input', 'categorical'\n",
        "        shuffle=True,\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    val_data_provider = datagen.flow_from_dataframe(\n",
        "        df,\n",
        "        data_dir,\n",
        "        x_col=\"id\",\n",
        "        y_col=y,\n",
        "        color_mode=color_mode,\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=class_mode,#'input', 'categorical'\n",
        "        shuffle=True,\n",
        "        subset='validation'\n",
        "    )\n",
        "    if(class_mode==\"multi_output\"):\n",
        "        train_data_provider = MultiLabelImageDataGenerator(train_data_provider, articleType_encoder, color_encoder)\n",
        "        val_data_provider = MultiLabelImageDataGenerator(val_data_provider, articleType_encoder, color_encoder)\n",
        "\n",
        "    return train_data_provider, val_data_provider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQBPMlzs6xPs"
      },
      "source": [
        "## Frechet Inception Distance (FID)\n",
        "The main metric for evalution image quality is [**Frechet Inception Distance (FID)**](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance).\n",
        "\n",
        "FID compares the distribution of generated images with the distribution of real images. The first step involves calculating the feature vector of each image in each domain using the [InceptionNet v3](https://keras.io/api/applications/inceptionv3/) model. FID compares the mean and standard deviation of the gaussian distributions containing feature vectors obtained from the deepest layer in Inception v3.\n",
        "\n",
        "The final formula is\n",
        "$$d^2=||μ_1−μ_2||^2+Tr(C_1+C_2−2\\sqrt{C_1⋅C_2})$$\n",
        "Where:\n",
        "- The “$\\mu_1$” and “$\\mu_2$” refer to the feature-wise mean of the real and generated images, e.g. 2,048 element vectors where each element is the mean feature observed across the images.\n",
        "- The *$C_1$* and *$C_2$* are the covariance matrix for the real and generated feature vectors.\n",
        "- The *$||\\mu_1 – \\mu_2||^2$* refers to the sum squared difference between the two mean vectors.\n",
        "- *$Tr$* refers to the [trace linear algebra operation](https://en.wikipedia.org/wiki/Trace_(linear_algebra)?ref=blog.paperspace.com), e.g. the sum of the elements along the main diagonal of the square matrix.\n",
        "  \n",
        "Practical use of FID have some limitation because `InceptionV3` model accepts only square colorful images bigger than 75x75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvGQtQvT6xPs"
      },
      "outputs": [],
      "source": [
        "def getInceptionModel(image_shape):\n",
        "    inception_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                input_shape=image_shape,\n",
        "                                weights=\"imagenet\",\n",
        "                                pooling='avg')\n",
        "    return inception_model\n",
        "\n",
        "def compute_embeddings(dataloader, count, inception_model):\n",
        "    image_embeddings = []\n",
        "    it = iter(dataloader)\n",
        "    for _ in range(count):\n",
        "        images = next(it)\n",
        "        if(type(images) is tuple):\n",
        "            images, _ = images\n",
        "        embeddings = inception_model.predict(images, verbose=0)\n",
        "        image_embeddings.extend(embeddings)\n",
        "    return np.array(image_embeddings)\n",
        "\n",
        "def calculate_fid(real_embeddings, generated_embeddings):\n",
        "    # calculate mean and covariance statistics\n",
        "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
        "    mu2, sigma2 = generated_embeddings.mean(axis=0), np.cov(generated_embeddings,  rowvar=False)\n",
        "    # calculate sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "    # calculate sqrt of product between cov\n",
        "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    # calculate score\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return round(math.sqrt(fid),2)\n",
        "\n",
        "def compute_fid(real_img_generator, fake_img_generator, image_shape):\n",
        "    count = len(real_img_generator) - 1\n",
        "\n",
        "    inception_model = getInceptionModel(image_shape)\n",
        "\n",
        "    # compute embeddings for real images\n",
        "    real_image_embeddings = compute_embeddings(real_img_generator, count, inception_model)\n",
        "\n",
        "    # compute embeddings for generated images\n",
        "    generated_image_embeddings= compute_embeddings(fake_img_generator, count, inception_model)\n",
        "\n",
        "    if(len(generated_image_embeddings) > len(real_image_embeddings)):\n",
        "        #make two arrays equal\n",
        "        generated_image_embeddings = generated_image_embeddings[:len(real_image_embeddings)]\n",
        "\n",
        "    fid = calculate_fid(real_image_embeddings, generated_image_embeddings)\n",
        "    print(\"FID: \" + str(fid))\n",
        "    return fid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi84JgdA6xPt"
      },
      "source": [
        "### Autoencoder Image Generator\n",
        "To calculate FID, we wrap our image generator model into an iterator, enabling the generation of new images without specifying model inputs. The CCVAEImageGenerator is also valuable for generated image plotting.\n",
        "\n",
        "The variation in the generated images can be controlled by adjusting the `normal_variate_sigma` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL0IO8yO6xPt"
      },
      "outputs": [],
      "source": [
        "normal_variate_sigma = 0.7 # change generation image variation lavel\n",
        "\n",
        "class CCVAEImageGenerator:\n",
        "    def __init__(self, model, label_provider):\n",
        "        self.model = model\n",
        "        self.encoder_input_size = model.layers[0].input_shape[0][1]\n",
        "        self.label_provider = label_provider\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        inputs = []\n",
        "        labels = next(self.label_provider)\n",
        "        for _ in range(len(labels)):\n",
        "            random_sample = []\n",
        "            for _ in range(self.encoder_input_size):\n",
        "                random_sample.append(random.normalvariate(0, normal_variate_sigma))\n",
        "            inputs.append(random_sample)\n",
        "        generated_images = self.model.predict([np.array(inputs), np.array(labels)], verbose=0)\n",
        "\n",
        "        return generated_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdedkwRE6xPr"
      },
      "source": [
        "## Data provider creation\n",
        "The provided code contains several configurable variables that can be adjusted to customize the dataset provider for specific needs. Here's a summary of the variables and their impact on dataset customization:\n",
        "\n",
        "- `BATCH_SIZE`: Defines the number of images to be provided in a single iteration.\n",
        "- `image_heigh` and `image_weigh`: Determine the height and width of the images, collectively defining the image size.\n",
        "- `num_color_dimensions`: Specifies the number of color dimensions for images, where 1 represents grayscale and 3 represents RGB images.\n",
        "- `with_color_label`: A boolean variable that, when set to True, indicates that the one-hot encoded label should include both article type and color information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "UnOndGdO6xPr",
        "outputId": "b7c6dc4c-1dc2-40dd-a651-6bc5b16185a2"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "image_heigh = 80\n",
        "image_weigh = 80\n",
        "num_color_dimensions = 3 # 1 for greyscale or 3 for RGB\n",
        "with_color_label = True # class label inlude article color\n",
        "\n",
        "# Computed parameters\n",
        "image_size = (image_heigh, image_weigh)\n",
        "image_shape = (image_heigh, image_weigh, num_color_dimensions)\n",
        "num_pixels = image_heigh * image_weigh * num_color_dimensions\n",
        "rgb_on = (num_color_dimensions==3)\n",
        "is_fid_active = image_heigh == image_weigh and image_weigh > 75 and rgb_on #check FID constrains\n",
        "if(with_color_label and (not rgb_on)): # error check\n",
        "   raise Exception(\"Illegal state: color label can be used only with RGB images\")\n",
        "\n",
        "class_mode = \"multi_output\" if(with_color_label) else \"categorical\"\n",
        "train_provider, val_provider  = create_data_provider_df(\n",
        "    IMG_FOLDER,\n",
        "    class_mode=class_mode,\n",
        "    image_size=image_size,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    rgb=rgb_on,\n",
        ")\n",
        "one_hot_label_len = train_provider.num_classes if(with_color_label) else len(train_provider.class_indices)\n",
        "if(type(train_provider) is MultiLabelImageDataGenerator):\n",
        "    all_one_hot_labels = train_provider.labels\n",
        "else:\n",
        "    all_one_hot_labels = to_categorical(train_provider.labels)\n",
        "\n",
        "plot_provided_images(train_provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2EYx94F6xPt"
      },
      "source": [
        "# Autoencoder\n",
        "The chosen architectural solution for this problem is the Convolutional Conditional Variational Autoencoder (CCVAE). It consists of an Encoder and a Decoder, each with specific structural components.\n",
        "\n",
        "Encoder Structure:\n",
        "- The Encoder takes two inputs: one for the image with 1 or 3 filters and another for the image label, reshaped as the additional image filter.\n",
        "- It includes three downsampling blocks, each of which doubles the number of filters and reduces the image size by a factor of 2.\n",
        "- A 1x1 convolutional layer is employed to compress the number of filters by a factor of 4.\n",
        "- Another downsampling layer follows.\n",
        "- Finally, there are two fully connected layers in the Encoder.\n",
        "\n",
        "The latent space has a dimensionality of 32.\n",
        "\n",
        "The Decoder's structure is symmetric to the Encoder, using convolutional transpose layers for upsampling.\n",
        "\n",
        "Each downsampling and upsampling block use batch normalization.\n",
        "\n",
        "Additionally, there is a visual representation of the core layers of Encoder:\n",
        "\n",
        "![Alt text](https://github.com/DenGuzawr22/Fashion-Product-Images-Generation/blob/main/res/EncoderStructure.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkIyxWmy6xPt"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLikSX1M6xPt"
      },
      "outputs": [],
      "source": [
        "class CCVAE():\n",
        "    def __init__(self):\n",
        "       self.num_downsampling = 3\n",
        "\n",
        "    def build_ccvae(self, shape, dense_neurons, encoded_dim, hidden_activation, label_input_len):\n",
        "        #Input\n",
        "        encoder_input = layers.Input(shape=shape, name='encoder_img_input')\n",
        "        label_input = layers.Input(shape=(label_input_len,), name='encoder_label_input')\n",
        "        encoder_label = layers.Dense(shape[0]*shape[1], name=\"encoder_label_size_augmentation\")(label_input)\n",
        "        encoder_label = layers.Reshape((shape[0], shape[1], 1), name=\"encoder_label_reshape\")(encoder_label)\n",
        "        concatedated_input = layers.Concatenate(name=\"input_concatenator\")([encoder_input, encoder_label])\n",
        "\n",
        "        #Encoder\n",
        "        prev_layer, last_conv_shape = self.build_encoder(concatedated_input, hidden_activation, dense_neurons)\n",
        "\n",
        "        mu = layers.Dense(encoded_dim, name='mu')(prev_layer)\n",
        "        log_var = layers.Dense(encoded_dim, name='log_var')(prev_layer)\n",
        "\n",
        "        encoder = keras.Model([encoder_input, label_input], [mu, log_var], name='encoder')\n",
        "\n",
        "        #Decoder\n",
        "        decoder_input = layers.Input(shape=(encoded_dim,), name='decoder_input')\n",
        "        concatedated_input_dec = layers.Concatenate(name=\"decoder_concat_input\")([decoder_input, label_input])\n",
        "\n",
        "        prev_layer = self.build_decoder(concatedated_input_dec, hidden_activation, dense_neurons, last_conv_shape)\n",
        "        decoder_output_layer = layers.Conv2D(shape[2], 4, padding=\"same\", activation='sigmoid')(prev_layer)\n",
        "\n",
        "        decoder = keras.Model([decoder_input, label_input], decoder_output_layer, name='decoder')\n",
        "\n",
        "        #Sampling layer\n",
        "        s = layers.Lambda(self.sampling, output_shape=(encoded_dim,), name='s')([mu, log_var])\n",
        "\n",
        "        #VAE\n",
        "        vae=keras.Model(encoder.input, decoder([s, label_input]),name='ccvae')\n",
        "\n",
        "        return vae,encoder,decoder\n",
        "\n",
        "    def build_encoder(self, input, h_activation, dense_neurons):\n",
        "        prev_layer = input\n",
        "        channels = 32\n",
        "        prev_layer = self.create_conv_block(prev_layer, channels, h_activation, 4, norm=True)\n",
        "        for i in range(self.num_downsampling):\n",
        "            channels *= 2\n",
        "            prev_layer = self.create_downsampling_conv_block(prev_layer, channels, h_activation)\n",
        "\n",
        "        channels = channels/4\n",
        "        prev_layer = self.create_conv_block(prev_layer, channels, h_activation, 1, norm=True)\n",
        "\n",
        "        channels *= 2\n",
        "        prev_layer = self.create_downsampling_conv_block(prev_layer, channels, h_activation)\n",
        "\n",
        "        last_conv_shape = K.int_shape(prev_layer)\n",
        "\n",
        "        prev_layer = layers.Flatten(name=\"Flatten\")(prev_layer)\n",
        "\n",
        "        for neuron_count in dense_neurons:\n",
        "            prev_layer=layers.Dense(neuron_count,activation=h_activation)(prev_layer)\n",
        "\n",
        "        return prev_layer, last_conv_shape\n",
        "\n",
        "\n",
        "    def build_decoder(self, input, h_activation, dense_neurons, last_conv_shape):\n",
        "        prev_layer=input\n",
        "        for neuron_count in reversed(dense_neurons):\n",
        "            prev_layer=layers.Dense(neuron_count,activation=h_activation)(prev_layer)\n",
        "\n",
        "        n = last_conv_shape[1] * last_conv_shape[2] * last_conv_shape[3]\n",
        "        prev_layer = layers.Dense(n, activation=h_activation)(prev_layer)\n",
        "        prev_layer = layers.Reshape((last_conv_shape[1],last_conv_shape[2], last_conv_shape[3]))(prev_layer)\n",
        "        channels = last_conv_shape[3]\n",
        "\n",
        "        for i in range(self.num_downsampling):\n",
        "            channels //= 2\n",
        "            prev_layer = self.create_upsampling_conv_block(prev_layer, channels, h_activation)\n",
        "\n",
        "        channels = channels*4\n",
        "        prev_layer = self.create_conv_block(prev_layer, channels,h_activation, 1, norm=True)\n",
        "\n",
        "        channels //= 2\n",
        "        prev_layer = self.create_upsampling_conv_block(prev_layer, channels, h_activation)\n",
        "        return prev_layer\n",
        "\n",
        "\n",
        "    def create_conv_block(self, prev_layer, channels, activation, kernel_size=3, padding='same', norm=False):\n",
        "      prev_layer = layers.Conv2D(channels, kernel_size, padding=padding, use_bias=True)(prev_layer)\n",
        "      if(norm):\n",
        "          prev_layer = layers.BatchNormalization()(prev_layer)\n",
        "      prev_layer = layers.Activation(activation)(prev_layer)\n",
        "      return prev_layer\n",
        "\n",
        "    def create_downsampling_conv_block(self, prev_layer, channels, activation, kernel_size=3):\n",
        "        prev_layer = layers.ZeroPadding2D()(prev_layer)\n",
        "        prev_layer = layers.Conv2D(channels, kernel_size, strides=2, use_bias=False)(prev_layer)\n",
        "        prev_layer = layers.BatchNormalization()(prev_layer)\n",
        "        prev_layer = layers.Activation(activation)(prev_layer)\n",
        "        return prev_layer\n",
        "\n",
        "    def create_upsampling_conv_block(self, prev_layer, channels, activation, kernel_size = 3):\n",
        "        prev_layer = layers.Conv2DTranspose(channels, kernel_size,strides=2, padding=\"same\")(prev_layer)\n",
        "        prev_layer = layers.BatchNormalization()(prev_layer)\n",
        "        prev_layer = layers.Activation(activation)(prev_layer)\n",
        "        return prev_layer\n",
        "\n",
        "    def sampling(self, args):\n",
        "      mu, log_var = args\n",
        "      batch_size = K.shape(mu)[0]\n",
        "      dim = K.int_shape(mu)[1]\n",
        "      epsilon = K.random_normal(shape=(batch_size, dim), mean=0., stddev=1.0)\n",
        "      return K.exp(0.5 * log_var) * epsilon + mu\n",
        "\n",
        "\n",
        "def vae_loss(vae_input,vae_ouput,mu,log_var,kl_coefficient, input_count):\n",
        "  #Reconstruction loss\n",
        "  vae_input = vae_input[0] if(type(vae_input) is list) else vae_input\n",
        "  x = keras.layers.Reshape((input_count,))(vae_input)\n",
        "  y = keras.layers.Reshape((input_count,))(vae_ouput)\n",
        "  reconstruction_loss = keras.losses.mean_squared_error(x, y) * input_count\n",
        "\n",
        "  #Regularization loss\n",
        "  kl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)\n",
        "\n",
        "  #Combined loss\n",
        "  return reconstruction_loss + kl_coefficient*kl_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qwtOYbC6xPt"
      },
      "source": [
        "## Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5vAD9Rt6xPt",
        "outputId": "db9e4d4c-be56-4c19-a378-b84663d993e5"
      },
      "outputs": [],
      "source": [
        "latent_space_dimension = 32\n",
        "internal_dense_layers = [2048, 512]\n",
        "vae, vae_encoder, vae_decoder = CCVAE().build_ccvae(\n",
        "   image_shape,\n",
        "   internal_dense_layers,\n",
        "   latent_space_dimension,\n",
        "   'LeakyReLU',\n",
        "   one_hot_label_len)\n",
        "\n",
        "vae.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taHJGD8R6xPt"
      },
      "source": [
        "## Model visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfYpvcSW6xPu"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(vae, show_shapes=True, show_layer_names=True, expand_nested=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QJ2YDDe6xPu"
      },
      "source": [
        "## Model compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dTvUQAh6xPu"
      },
      "outputs": [],
      "source": [
        "### Compilation\n",
        "kl_coefficient=1\n",
        "\n",
        "#Information needed to compute the loss function\n",
        "vae_input=vae.input\n",
        "vae_output=vae.output\n",
        "mu=vae.get_layer('mu').output\n",
        "log_var=vae.get_layer('log_var').output\n",
        "\n",
        "vae.add_loss(vae_loss(vae_input, vae_output, mu, log_var, kl_coefficient, num_pixels))\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam(clipnorm=0.001))\n",
        "\n",
        "loss_metrics=[]\n",
        "val_loss_metrics=[]\n",
        "fid_frequency_metrics = []\n",
        "\n",
        "# Get a batch from validation set for ploting model generalization capabilities\n",
        "NUM_IMAGES_TO_PLOT = 15\n",
        "if(NUM_IMAGES_TO_PLOT > BATCH_SIZE):\n",
        "   raise Exception(\"Can not plot more images than there are in a batch\")\n",
        "val_x, val_y = next(val_provider)\n",
        "if (len(val_x) < NUM_IMAGES_TO_PLOT):\n",
        "   val_x, val_y = next(val_provider) # redo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIe-fA2R6xPu"
      },
      "source": [
        "## Training\n",
        "This code manages the training and evaluation of a CCVAE model over a specified number of epochs\n",
        "\n",
        "Key points include:\n",
        "- `epoch_count` indicating the number of training iterations.\n",
        "- `image_plot_frequency` controls how often input-output comparisons are plotted.\n",
        "- `fid_frequency` determines how often the Frechet Inception Distance (FID) is calculated.\n",
        "\n",
        "This code manages the training process, monitors loss, visualizes results, and optionally calculates FID during the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HCEXHS1B6xPu",
        "outputId": "993fc615-c6a9-4282-d78b-f9c04ba3ea2b"
      },
      "outputs": [],
      "source": [
        "epoch_count = 4\n",
        "image_plot_frequency = 1 # how often input-output comparation should be ploted\n",
        "fid_frequency = 8 # how often fid should be calculated\n",
        "\n",
        "def batch_eleboration(model, generator, validation=False):\n",
        "   n = 0\n",
        "   loss_sum = 0\n",
        "   for _ in range(len(generator)):\n",
        "      batch_x, batch_y = next(generator)\n",
        "      if(validation):\n",
        "         loss = model.test_on_batch([batch_x, batch_y], batch_x)\n",
        "      else:\n",
        "         loss = model.train_on_batch([batch_x, batch_y], batch_x)\n",
        "      n += len(batch_x)\n",
        "      loss_sum += (loss*len(batch_x))\n",
        "   return loss_sum / n\n",
        "\n",
        "for e in range(1, epoch_count+1):\n",
        "   start_time = time.time()\n",
        "\n",
        "   loss = batch_eleboration(vae, train_provider)\n",
        "   loss_metrics.append(loss)\n",
        "\n",
        "   val_loss = batch_eleboration(vae, val_provider, validation=True)\n",
        "   val_loss_metrics.append(val_loss)\n",
        "\n",
        "   end_time = time.time()\n",
        "   print('Epoch: {0} exec_time={1:.1f}s  loss={2:.3f} val_loss={3:.3f}'.format(e,end_time - start_time, loss, val_loss))\n",
        "\n",
        "   if(e%image_plot_frequency == 0):\n",
        "      plot_same_model_input_and_output(val_x, val_y, vae, num=NUM_IMAGES_TO_PLOT)\n",
        "      #plot_model_input_and_output(val_provider, vae)\n",
        "   if(is_fid_active and e%fid_frequency == 0):\n",
        "      image_generator = CCVAEImageGenerator(vae_decoder, labels_provider(all_one_hot_labels, BATCH_SIZE))\n",
        "      fid_frequency_metrics.append(compute_fid(train_provider, image_generator, image_shape))\n",
        "\n",
        "plot_model_input_and_output(val_provider, vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLhCp3OG6xPu"
      },
      "source": [
        "## Performance evaluation\n",
        "\n",
        "\n",
        "To calculate fid for each real label is generated one fake image it allows to get average $stddev(fid^2) = ~4.5$. If we generate images with random labels we will get $stddev(fid^2) = ~50$ that not allow to use fid as a stable metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ho8NwEf36xPu",
        "outputId": "341385e9-262e-4d63-d8de-fd399baae161"
      },
      "outputs": [],
      "source": [
        "plot_losses_from_array(loss_metrics,val_loss_metrics)\n",
        "\n",
        "if(is_fid_active):\n",
        "   plot_fid(fid_frequency_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byXgW7dt6xPu"
      },
      "source": [
        "## Generate images\n",
        "This code facilitates the visualization of generated images. When color labels are not in use, it generates a row of images for each article type. If color labels are used, it creates a row for each article type, with each column dedicated to a specific color variation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "L6I7cotj6xPu",
        "outputId": "31e4540d-831d-42dc-f32e-9506a3fcf5d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_model_generated_article_types(model, one_hot_len, rows=1, cols=5, imgProducer=CCVAEImageGenerator):\n",
        "  for i in range(one_hot_len):\n",
        "    one_hot = np.zeros(one_hot_len, dtype=float)\n",
        "    one_hot[i] = 1\n",
        "    one_hots = [one_hot] * cols\n",
        "    decoderGen = imgProducer(model, label_provider(one_hots))\n",
        "    iterator = iter(decoderGen)\n",
        "\n",
        "    generated_images=[]\n",
        "    for _ in range(rows):\n",
        "        generated_images.append(next(iterator))\n",
        "\n",
        "    plot_generated_images(generated_images,rows,cols)\n",
        "\n",
        "def plot_model_generated_colorfull_article_types(model, num_classes, one_hot_len, rows=1, imgProducer=CCVAEImageGenerator):\n",
        "  num_colors = one_hot_len - num_classes\n",
        "  for clas in range(num_classes):\n",
        "    one_hots = []\n",
        "    for color in range(num_classes, one_hot_len):\n",
        "        one_hot = np.zeros(one_hot_len, dtype=float)\n",
        "        one_hot[clas] = 1\n",
        "        one_hot[color] = 1\n",
        "        one_hots.append(one_hot)\n",
        "    decoderGen = imgProducer(model, label_provider(one_hots))\n",
        "\n",
        "    for _ in range(rows):\n",
        "      plot_generated_images([next(iter(decoderGen))],1,num_colors)\n",
        "\n",
        "if(with_color_label):\n",
        "  plot_model_generated_colorfull_article_types(vae_decoder, len(CLASSES), one_hot_label_len, rows=1)\n",
        "else:\n",
        "  plot_model_generated_article_types(vae_decoder, one_hot_label_len, rows=1, cols=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxHPD2at6xPv"
      },
      "source": [
        "# Results\n",
        "In the final training phase, two neural networks were trained. One network takes color labels as input, while the other does not. Additional important details:\n",
        "- The training involved 9 distinct article types.\n",
        "- FID measurement was performed every 8 epochs.\n",
        "- FID of not trained decoder is ~58\n",
        "- FID after one epoch ~38\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Ydx-s46xPv"
      },
      "source": [
        "### Without color labels\n",
        "Throughout the training process, the FID metric steadily decreased from an initial value of 32.5 to approximately 22, indicating improved image generation quality.\n",
        "\n",
        "![](https://github.com/DenGuzawr22/Fashion-Product-Images-Generation/blob/main/res/vae_nocolor_fid.png?raw=true)\n",
        "\n",
        "As a result of this progress, the model demonstrated its capability to generate the following images:\n",
        "\n",
        "![](https://github.com/DenGuzawr22/Fashion-Product-Images-Generation/blob/main/res/vae_nocolor_images.png?raw=true)\n",
        "\n",
        "\n",
        "### With color labels\n",
        "Throughout the training process, the FID metric steadily decreased from an initial value of 33 to approximately 24. That is a little bit worse than results of the model without color labels. It can be caused of wrong color labels and presense multi-color object.\n",
        "\n",
        "![](https://github.com/DenGuzawr22/Fashion-Product-Images-Generation/blob/main/res/vae_fid.png?raw=true)\n",
        "\n",
        "As a result, the model demonstrated its capability to generate the following images:\n",
        "\n",
        "![](https://github.com/DenGuzawr22/Fashion-Product-Images-Generation/blob/main/res/vae_images_main.png?raw=true)\n",
        "\n",
        "More results in project presentation..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
