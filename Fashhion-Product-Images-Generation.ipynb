{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion Product Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will present the deep learning generative models that can generate new images of fashion products. \n",
    "\n",
    "Main objects of the project: Given an article type and a color the model must be able to generate a corresponding image.\n",
    "\n",
    "We will use [**TensorFlow**](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and [**Keras**](https://keras.io/) open-source library for neural network development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful models import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import importlib\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy import linalg\n",
    "from keras import layers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "Execute the following code to define some utility functions used in this project:\n",
    "- `get_dataset_folder_file_path`: given file name returns its path \n",
    "- `get_image_path`: given image id returns its path\n",
    "- `labels_provider`: given a list with one hot encoded labels and batch size, returns a supplier that gives one batch of data from the list in one iteration.\n",
    "- `infinite_generator`: used by label infinite lambda provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset/\"\n",
    "\n",
    "IMG_FOLDER = DATASET_PATH + \"images/images/\"\n",
    "BW_IMG_FOLDER = DATASET_PATH + \"bw/\"\n",
    "BW_IMG_FOLDER_INNER = BW_IMG_FOLDER + \"img/\"\n",
    "\n",
    "COLOR_IMG_FOLDER = DATASET_PATH + \"color/\"\n",
    "\n",
    "def get_dataset_folder_file_path(file_name):\n",
    "    return DATASET_PATH + file_name\n",
    "\n",
    "def get_image_path(image_id, folder=IMG_FOLDER):\n",
    "    return folder + str(image_id) + \".jpg\"\n",
    "\n",
    "def labels_provider(l, n): \n",
    "   while len(l) > 0:\n",
    "      poped = l[:n]\n",
    "      l = l[n:]\n",
    "      yield poped\n",
    "\n",
    "def infinite_generator(value):\n",
    "    while True:\n",
    "        yield value\n",
    "\n",
    "label_provider = lambda a: infinite_generator(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploters \n",
    "Execute the following code to define the utility functions used for image and chart plotting:\n",
    "- `plot_images_by_id`: plot images given a list of ids\n",
    "- `plot_random_images`: plot images given a pandas dataframe\n",
    "- `plot_generated_images`: plot images given list with image pixels data\n",
    "- `plot_provided_images`: plot images given a image provider\n",
    "- `plot_model_input_and_output`: plot input and output images of an autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids (list): is a list of image id, the number of ids must be multiple of 5\n",
    "def plot_images_by_id(ids, folder=IMG_FOLDER):\n",
    "    rows = int(len(ids)/5)\n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(15, rows*3))\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        image_id = ids[i]\n",
    "        image_path = get_image_path(image_id, folder)  \n",
    "        img = Image.open(image_path)\n",
    "        ax.imshow(img, cmap='Greys_r')\n",
    "        ax.set_title(image_id)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_random_images(df, num=5, path=IMG_FOLDER):\n",
    "  \"\"\" plot random 15 images from dataframe\n",
    "\n",
    "  Args:\n",
    "      df (dataframe): pandas dataframe\n",
    "      num (int, optional): number of images. Defaults to 15.\n",
    "      path (string, optional): path to the folder with images. Defaults to paths.IMG_FOLDER.\n",
    "  \"\"\"\n",
    "  ids = df['id']\n",
    "  selected_image_ids = random.sample(ids.tolist(), num)\n",
    "  plot_images_by_id(selected_image_ids, path)\n",
    "\n",
    "\n",
    "def plot_generated_images(generated_images, nrows, ncols, no_space_between_plots=False, figsize=(15, 15)):\n",
    "  _, axs = plt.subplots(nrows, ncols, figsize=figsize,squeeze=False)\n",
    "\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      axs[i,j].axis('off')\n",
    "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
    "\n",
    "  if no_space_between_plots:\n",
    "    plt.subplots_adjust(wspace=0,hspace=0)\n",
    "\n",
    "  plt.show()\n",
    "  \n",
    "def plot_provided_images(provider):\n",
    "    it = next(provider)\n",
    "    if(type(it) is tuple):\n",
    "       images, _ = it\n",
    "    else: \n",
    "        images = it \n",
    "\n",
    "    print(\"An image shape: \", images[1].shape)\n",
    "    plot_generated_images([images], 1, 5)\n",
    "\n",
    "    print(\"Images shape (numImages, high, width, numColors):\")\n",
    "    print(images.shape)\n",
    "\n",
    "# Used for showing autoencoder input and its corresponding output\n",
    "def plot_model_input_and_output(generator, model, num=6):\n",
    "   # Trasform 5 random images from validation set\n",
    "   val_x, val_y = next(generator)\n",
    "   if (len(val_x) < num):\n",
    "      val_x, val_y = next(generator) # redo \n",
    "\n",
    "   # get first 5 dataset images\n",
    "   real_imgs = val_x[:num] \n",
    "   labels = val_y[:num]\n",
    "   plot_generated_images([real_imgs], 1, num)\n",
    "\n",
    "   generated_imgs = model.predict([real_imgs,labels], verbose=0)\n",
    "   plot_generated_images([generated_imgs], 1, num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History ploters\n",
    "Execute the following code to define the utility functions used for model history loss changes chart plotting:\n",
    "- `plot_fid`: plot Frechet Inception Distance by checkpoints\n",
    "- `plot_losses_from_array`: plot trainig and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fid(fid_values):\n",
    "  checkpoints = range(1, len(fid_values) + 1)\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(checkpoints, fid_values, marker='o', linestyle='-')\n",
    "  plt.title('FID Changes by Checkpoint')\n",
    "  plt.xlabel('Checkpoint')\n",
    "  plt.ylabel('FID Value')\n",
    "  plt.grid(True)\n",
    "\n",
    "\n",
    "def plot_losses_from_array(training_losses, validation_losses):\n",
    "  epochs = list(range(1, len(training_losses) + 1))\n",
    "  plt.plot(epochs, training_losses, label='Training Loss',  linestyle='-')\n",
    "  plt.plot(epochs, validation_losses, label='Validation Loss', linestyle='-')\n",
    "\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training and Validation Loss Over Epochs')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The [**Fashion Product Images (Small)**](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small?select=styles.csv) dataset, containing 80x60 colorful images of fashion products. Original dataset have 44000 products\n",
    "\n",
    "There are 143 article types half of them have less than 50 articles. We choose 12 the most stable and homogeneous article types as our working target: *Watches*, *Handbags*, *Sunglasses*, *Belts*, *Backpacks*, *Sarees*, *Deodorant*, *Nail Polish*, *Ties*, *Sports Shoes*, *Flip Flops*, *Formal Shoes*. Those 12 classes contains circa 12000 of products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Possible classes:\n",
    "\n",
    "# \"Watches\" \n",
    "# \"Handbags\" \n",
    "# \"Sunglasses\" \n",
    "# \"Belts\" \n",
    "# \"Backpacks\"\n",
    "# \"Sarees\" \n",
    "# \"Deodorant\" \n",
    "# \"Nail Polish\" \n",
    "# \"Ties\" \n",
    "# \"Sports Shoes\" \n",
    "# \"Flip Flops\" \n",
    "# \"Formal Shoes\" \n",
    "\n",
    "# Flexible list of the article types that will be elaborated by neural networks\n",
    "CLASSES = [\"Watches\", \"Handbags\", \"Sunglasses\", \"Belts\", \"Flip Flops\"]\n",
    "\n",
    "def filter_articles(df, classes):\n",
    "    return df[df['articleType'].isin(classes)]\n",
    "\n",
    "def get_dataframe_by(df, column_name, value):\n",
    "    return df[df[column_name] == value]\n",
    "\n",
    "def get_dataframe_by_article_type(df, article_type):\n",
    "    return get_dataframe_by(df, \"articleType\", article_type)\n",
    "\n",
    "def get_dataframe_by_color(df, color):\n",
    "    return get_dataframe_by(df, \"baseColour\", color)\n",
    "\n",
    "\n",
    "df = pd.read_csv(get_dataset_folder_file_path('styles.csv'), dtype=str)\n",
    "df = df[df.notnull()[\"baseColour\"]] # remove null values from basecolor column\n",
    "\n",
    "# df['articleType'].value_counts()\n",
    "df = filter_articles(df, CLASSES)\n",
    "\n",
    "for t in df['articleType'].unique():\n",
    "    plot_random_images(get_dataframe_by_article_type(df, t))\n",
    "\n",
    "df['articleType'].value_counts() # number of products for each article type\n",
    "#todo blacklist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color aggregation\n",
    "In dataset the products have 42 different colors. To be able generate images of a specific color in a stable way we should aggregate similar colors. After aggregation our articles will have 10 colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseColour = \"baseColour\"\n",
    "print(\"before aggreation number of colors: \", len(df[baseColour].value_counts()))\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Lime Green\", \"Green\")\n",
    "df[baseColour] = df[baseColour].replace(\"Fluorescent Green\", \"Green\")\n",
    "df[baseColour] = df[baseColour].replace(\"Sea Green\", \"Green\")\n",
    "df[baseColour] = df[baseColour].replace(\"Teal\", \"Green\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Taupe\", \"Grey\")\n",
    "df[baseColour] = df[baseColour].replace(\"Grey Melange\", \"Grey\")\n",
    "df[baseColour] = df[baseColour].replace(\"Steel\", \"Grey\")\n",
    "df[baseColour] = df[baseColour].replace(\"Silver\", \"Grey\")\n",
    "df[baseColour] = df[baseColour].replace(\"Skin\", \"Beige\")\n",
    "df[baseColour] = df[baseColour].replace(\"Grey\", \"White\") # White + Grey\n",
    "df[baseColour] = df[baseColour].replace(\"Beige\", \"White\") # White + Beige\n",
    "df[baseColour] = df[baseColour].replace(\"Off White\", \"White\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Mushroom Brown\", \"Brown\")\n",
    "df[baseColour] = df[baseColour].replace(\"Nude\", \"Brown\")\n",
    "df[baseColour] = df[baseColour].replace(\"Coffee Brown\", \"Brown\")\n",
    "df[baseColour] = df[baseColour].replace(\"Burgundy\", \"Brown\")\n",
    "df[baseColour] = df[baseColour].replace(\"Copper\", \"Brown\")\n",
    "df[baseColour] = df[baseColour].replace(\"Bronze\", \"Brown\")\n",
    "df[baseColour] = df[baseColour].replace(\"Tan\", \"Brown\")\n",
    "df[baseColour] = df[baseColour].replace(\"Khaki\", \"Brown\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Rose\", \"Red\")\n",
    "df[baseColour] = df[baseColour].replace(\"Orange\", \"Red\")\n",
    "df[baseColour] = df[baseColour].replace(\"Rust\", \"Red\")\n",
    "df[baseColour] = df[baseColour].replace(\"Maroon\", \"Red\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Magenta\", \"Pink\")\n",
    "df[baseColour] = df[baseColour].replace(\"Peach\", \"Pink\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Mauve\", \"Purple\")\n",
    "df[baseColour] = df[baseColour].replace(\"Lavender\", \"Purple\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Metallic\", \"Black\")\n",
    "df[baseColour] = df[baseColour].replace(\"Charcoal\", \"Black\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Turquoise Blue\", \"Blue\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Mustard\", \"Yellow\")\n",
    "df[baseColour] = df[baseColour].replace(\"Gold\", \"Yellow\")\n",
    "\n",
    "df[baseColour] = df[baseColour].replace(\"Cream\", \"Multi\")\n",
    "df[baseColour] = df[baseColour].replace(\"Olive\", \"Multi\")\n",
    "df[baseColour] = df[baseColour].replace(\"Navy Blue\", \"Multi\")\n",
    "\n",
    "print(\"After aggreation number of colors: \", len(df[baseColour].value_counts()))\n",
    "\n",
    "for color in df[baseColour].unique():\n",
    "    print(color)\n",
    "    plot_random_images(get_dataframe_by_color(df, color))\n",
    "\n",
    "df[baseColour].value_counts() # number of products for each color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading and preprocessing\n",
    "`MultiLabelImageDataGenerator` is an adapter to a [`DataFrameIterator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_dataframe). What its purpuse?\n",
    "The output of *DataFrameIterator* is:\n",
    "- In case of \"categorical\" single label configuration it is a tuple where first element is array of images and second array of **one hot encoded** labels\n",
    "    - Example: `([img_1, img_2], [one_hot_1, one_hot_2])`\n",
    "- In case of \"multi_label\" class mode configuration where are used multiples label it is a tuple where first element is array of images but the second element is an array that containst multiple arrays each of them contains all labels in **string format**. So the one hot encoding should be implemented additionally.\n",
    "    - Example `([img_1, img_2], [[\"type_1\", \"type_2\"], [\"color_1\", \"color_2\"]])`\n",
    "  \n",
    "`MultiLabelImageDataGenerator` aims to hide the process of one hot encoding and concatenation of multiple one hot encoding labels (this allows to use the same network for different number of labels). As the result we obtain the same output format for \"categorical\" and \"mutiple_label\" configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelImageDataGenerator:\n",
    "    def __init__(self, generator, articleType_encoder, color_encoder):\n",
    "        self.generator = generator\n",
    "        self.articleType_encoder = articleType_encoder\n",
    "        self.color_encoder = color_encoder\n",
    "        self.articl_n_classes =  len(articleType_encoder.classes_)\n",
    "        self.color_n_classes =  len(color_encoder.classes_)\n",
    "        self.num_classes = self.articl_n_classes + self.color_n_classes \n",
    "        self.class_indicies = articleType_encoder.classes_\n",
    "\n",
    "        all_artcle = to_categorical(articleType_encoder.transform(generator.labels[0]))\n",
    "        all_colors = to_categorical(color_encoder.transform(generator.labels[1]))\n",
    "\n",
    "        concatenated = []\n",
    "        for i in range(len(all_artcle)):\n",
    "            concatenated.append(all_artcle[i].tolist() + all_colors[i].tolist())\n",
    "        \n",
    "        self.labels = np.array(concatenated, dtype=np.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        x, y = next(self.generator)\n",
    "        articleType_one_hot = to_categorical(self.articleType_encoder.transform(y[0]), num_classes=self.articl_n_classes)\n",
    "        color_one_hot = to_categorical(self.color_encoder.transform(y[1]), num_classes=self.color_n_classes)\n",
    "        concatenated = []\n",
    "        for i in range(len(articleType_one_hot)):\n",
    "            concatenated.append(articleType_one_hot[i].tolist() + color_one_hot[i].tolist())\n",
    "        \n",
    "        return x, np.array(concatenated, dtype=np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data provider definition\n",
    "- `create_data_provider_df`: create a new provider of dataset images\n",
    "    - Args:\n",
    "      - `data_dir` (string): path to the directory with images\n",
    "      - `classes` (list of strings): list with article types that should be uploaded \n",
    "      - `class_mode` (str):\n",
    "        - \"categorical\" if one hot encoded label should contain only articleType.\n",
    "        - \"multi_output\" if one hot encoded label should contain articleType and baseColour.\n",
    "      - `batch_size` (int, optional): Number of images uploaded at each iteration. Defaults to 64.\n",
    "      - `image_size` (tuple, optional): image size. Defaults to (80,60).\n",
    "      - `rgb` (bool, optional): true if images should be with colors. Defaults to False.\n",
    "    - Returns:\n",
    "      - `MultiLabelImageDataGenerator`: for \"multi_output\"  `class_mode`\n",
    "      - `DataFrameIterator`: for \"categorical\" and others `class_mode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_provider_df(data_dir, df, class_mode,batch_size=64, image_size = (80,60), rgb=False):\n",
    "    color_mode =  \"rgb\" if (rgb) else \"grayscale\" \n",
    "    y = [\"articleType\", \"baseColour\"] if(class_mode==\"multi_output\") else \"articleType\"\n",
    "\n",
    "    def append_ext(id): return id+\".jpg\"\n",
    "    df['id'] = df['id'].apply(append_ext)\n",
    "    \n",
    "    articleType_encoder = LabelEncoder()\n",
    "    color_encoder = LabelEncoder()\n",
    "    articleType_encoder.fit(df[\"articleType\"].unique())\n",
    "    color_encoder.fit(df[\"baseColour\"].unique())\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1.0 / 255.0,  # Scale pixel values between 0 and 1\n",
    "        validation_split=0.1,\n",
    "    ) \n",
    "\n",
    "    train_data_provider = datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        data_dir,\n",
    "        x_col=\"id\",\n",
    "        y_col=y,\n",
    "        color_mode=color_mode,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode,#'input', 'categorical' \n",
    "        shuffle=True, \n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    val_data_provider = datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        data_dir,\n",
    "        x_col=\"id\",\n",
    "        y_col=y,\n",
    "        color_mode=color_mode,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode,#'input', 'categorical' \n",
    "        shuffle=True,\n",
    "        subset='validation'\n",
    "    )\n",
    "    if(class_mode==\"multi_output\"):\n",
    "        train_data_provider = MultiLabelImageDataGenerator(train_data_provider, articleType_encoder, color_encoder)\n",
    "        val_data_provider = MultiLabelImageDataGenerator(val_data_provider, articleType_encoder, color_encoder)\n",
    "    \n",
    "    return train_data_provider, val_data_provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data provider creatin\n",
    "We have several configurable variables for dataset provider personalizzation:\n",
    "- `BATCH_SIZE` difine how many images will be provided in one iteration\n",
    "- `image_heigh` and `image_weigh` defines the size of images\n",
    "- `num_color_dimensions` 1 for greyscale or 3 for RGB images\n",
    "- `with_color_label` if true the one hot encoded label contains together article type and color information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "image_heigh = 80\n",
    "image_weigh = 80\n",
    "num_color_dimensions = 3 # 1 for greyscale or 3 for RGB\n",
    "with_color_label = True # class label inlude article color\n",
    "\n",
    "# Computed parameters\n",
    "image_size = (image_heigh, image_weigh)\n",
    "image_shape = (image_heigh, image_weigh, num_color_dimensions)\n",
    "num_pixels = image_heigh * image_weigh * num_color_dimensions\n",
    "rgb_on = (num_color_dimensions==3)\n",
    "is_fid_active = image_heigh == image_weigh and image_weigh > 75 and rgb_on\n",
    "if(with_color_label and (not rgb_on)): # error check\n",
    "   raise Exception(\"Illegal state: color label can be used only with RGB images\")\n",
    "\n",
    "class_mode = \"multi_output\" if(with_color_label) else \"categorical\"\n",
    "train_provider, val_provider  = create_data_provider_df(\n",
    "    IMG_FOLDER,\n",
    "    df,\n",
    "    class_mode=class_mode,\n",
    "    image_size=image_size,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    rgb=rgb_on,\n",
    ")\n",
    "one_hot_label_len = train_provider.num_classes if(with_color_label) else len(train_provider.class_indices)\n",
    "if(type(train_provider) is MultiLabelImageDataGenerator):\n",
    "    all_one_hot_labels = train_provider.labels\n",
    "else:\n",
    "    all_one_hot_labels = to_categorical(train_provider.labels)\n",
    "\n",
    "plot_provided_images(train_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frechet Inception Distance (FID)\n",
    "As main metric for evalution image quality we choose [**Frechet Inception Distance (FID)**](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance). \n",
    "\n",
    "FID compares the distribution of generated images with the distribution of real images. The first step involves calculating the feature vector of each image in each domain using the [InceptionNet v3](https://keras.io/api/applications/inceptionv3/) model. FID compares the mean and standard deviation of the gaussian distributions containing feature vectors obtained from the deepest layer in Inception v3.\n",
    "\n",
    "The final formula is\n",
    "$$d^2=||μ_1−μ_2||^2+Tr(C_1+C_2−2\\sqrt{C_1⋅C_2})$$\n",
    "Where:\n",
    "- The “$\\mu_1$” and “$\\mu_2$” refer to the feature-wise mean of the real and generated images, e.g. 2,048 element vectors where each element is the mean feature observed across the images.\n",
    "- The *$C_1$* and *$C_2$* are the covariance matrix for the real and generated feature vectors.\n",
    "- The *$||\\mu_1 – \\mu_2||^2$* refers to the sum squared difference between the two mean vectors. *$Tr$* refers to the [trace linear algebra operation](https://en.wikipedia.org/wiki/Trace_(linear_algebra)?ref=blog.paperspace.com), e.g. the sum of the elements along the main diagonal of the square matrix.\n",
    "- The $sqrt$ is the square root of the square matrix\n",
    "\n",
    "Practical use of FID have some limitation because `InceptionV3` model accepts only colorful images bigger than 75x75 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInceptionModel(image_shape):\n",
    "    inception_model = tf.keras.applications.InceptionV3(include_top=False, \n",
    "                                input_shape=image_shape,\n",
    "                                weights=\"imagenet\", \n",
    "                                pooling='avg')\n",
    "    return inception_model\n",
    "\n",
    "def compute_embeddings(dataloader, count, inception_model):\n",
    "    image_embeddings = []\n",
    "    it = iter(dataloader)\n",
    "    for _ in range(count):\n",
    "        images = next(it)\n",
    "        if(type(images) is tuple):\n",
    "            images, _ = images\n",
    "        embeddings = inception_model.predict(images, verbose=0)\n",
    "        image_embeddings.extend(embeddings)\n",
    "    return np.array(image_embeddings)\n",
    "\n",
    "def calculate_fid(real_embeddings, generated_embeddings):\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
    "    mu2, sigma2 = generated_embeddings.mean(axis=0), np.cov(generated_embeddings,  rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return round(math.sqrt(fid),2)\n",
    "\n",
    "def compute_fid(real_img_generator, fake_img_generator, image_shape ):\n",
    "    count = len(real_img_generator) - 1\n",
    "\n",
    "    inception_model = getInceptionModel(image_shape)\n",
    "\n",
    "    # compute embeddings for real images\n",
    "    real_image_embeddings = compute_embeddings(real_img_generator, count, inception_model)\n",
    "\n",
    "    # compute embeddings for generated images\n",
    "    generated_image_embeddings= compute_embeddings(fake_img_generator, count, inception_model)\n",
    "\n",
    "    if(len(generated_image_embeddings) > len(real_image_embeddings)):\n",
    "        #make two arrays equal\n",
    "        generated_image_embeddings = generated_image_embeddings[:len(real_image_embeddings)]\n",
    "\n",
    "    fid = calculate_fid(real_image_embeddings, generated_image_embeddings)\n",
    "    print(\"FID: \" + str(fid))\n",
    "    return fid\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional image generator\n",
    "For FID calculation we need to wrap our gerator model into iterator, so we will can get new generated images without specifing model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalImageGeneratorDecoder:\n",
    "    def __init__(self, model, label_provider):\n",
    "        self.model = model\n",
    "        self.encoder_input_size = model.layers[0].input_shape[0][1]\n",
    "        self.label_provider = label_provider\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        inputs = []\n",
    "        labels = next(self.label_provider)\n",
    "        for _ in range(len(labels)):\n",
    "            random_sample = []\n",
    "            for _ in range(self.encoder_input_size):\n",
    "                random_sample.append(random.normalvariate(0,0.6))\n",
    "            inputs.append(random_sample)\n",
    "        generated_images = self.model.predict([np.array(inputs), np.array(labels)], verbose=0)\n",
    "        \n",
    "        return generated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "As architecture for this problem resolution was choosen Convulutional Conditional Variational Autoencoder (CCVAE)\n",
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "class CCVAE():\n",
    "    def __init__(self):\n",
    "       self.num_downsampling = 4\n",
    "\n",
    "    def build_ccvae(self, shape, dense_neurons, encoded_dim, hidden_activation, label_input_len):\n",
    "        #Input\n",
    "        encoder_input = layers.Input(shape=shape, name='encoder_img_input')\n",
    "        label_input = layers.Input(shape=(label_input_len,), name='encoder_label_input')\n",
    "        encoder_label = layers.Dense(shape[0]*shape[1], name=\"encoder_label_size_augmentation\")(label_input)\n",
    "        encoder_label = layers.Reshape((shape[0], shape[1], 1), name=\"encoder_label_reshape\")(encoder_label)\n",
    "        concatedated_input = layers.Concatenate(name=\"input_concatenator\")([encoder_input, encoder_label])\n",
    "\n",
    "        #Encoder\n",
    "        prev_layer, last_conv_shape = self.build_encoder(concatedated_input, hidden_activation, dense_neurons)\n",
    "        \n",
    "        mu = layers.Dense(encoded_dim, name='mu')(prev_layer)\n",
    "        log_var = layers.Dense(encoded_dim, name='log_var')(prev_layer)\n",
    "\n",
    "        encoder = keras.Model([encoder_input, label_input], [mu, log_var], name='encoder')\n",
    "\n",
    "        #Decoder\n",
    "        decoder_input = layers.Input(shape=(encoded_dim,), name='decoder_input')\n",
    "        concatedated_input_dec = layers.Concatenate(name=\"decoder_concat_input\")([decoder_input, label_input])\n",
    "\n",
    "        prev_layer = self.build_decoder(concatedated_input_dec, hidden_activation, dense_neurons, last_conv_shape)\n",
    "        decoder_output_layer = layers.Conv2D(shape[2], 4, padding=\"same\", activation='sigmoid')(prev_layer)\n",
    "\n",
    "        decoder = keras.Model([decoder_input, label_input], decoder_output_layer, name='decoder')\n",
    "\n",
    "        #Sampling layer\n",
    "        s = layers.Lambda(self.sampling, output_shape=(encoded_dim,), name='s')([mu, log_var])\n",
    "\n",
    "        #VAE\n",
    "        vae=keras.Model(encoder.input, decoder([s, label_input]),name='ccvae')\n",
    "\n",
    "        return vae,encoder,decoder\n",
    "    \n",
    "    def build_encoder(self, input, h_activation, dense_neurons):\n",
    "\n",
    "      prev_layer = input\n",
    "      channels = 16\n",
    "      prev_layer = self.create_conv_block(prev_layer, channels, h_activation, 4, norm=True)\n",
    "      for i in range(self.num_downsampling):\n",
    "          channels *= 2 \n",
    "          prev_layer = self.create_downsampling_conv_block(prev_layer, channels, h_activation)\n",
    "      \n",
    "      channels = channels/2\n",
    "      prev_layer = self.create_conv_block(prev_layer, channels,h_activation, 1, norm=True)\n",
    "\n",
    "      last_conv_shape = K.int_shape(prev_layer)\n",
    "\n",
    "      prev_layer = layers.Flatten(name=\"Flatten\")(prev_layer)\n",
    "      \n",
    "      for neuron_count in dense_neurons:\n",
    "          hidden_layer=layers.Dense(neuron_count,activation=h_activation)(prev_layer)\n",
    "          prev_layer=hidden_layer\n",
    "      \n",
    "      return prev_layer, last_conv_shape\n",
    "        \n",
    "\n",
    "    def build_decoder(self, input, h_activation, dense_neurons, last_conv_shape):\n",
    "        prev_layer=input\n",
    "        for neuron_count in reversed(dense_neurons):\n",
    "            hidden_layer=layers.Dense(neuron_count,activation=h_activation)(prev_layer)\n",
    "            prev_layer=hidden_layer\n",
    "    \n",
    "        n = last_conv_shape[1] * last_conv_shape[2] * last_conv_shape[3]\n",
    "        prev_layer = layers.Dense(n, activation=h_activation)(prev_layer)\n",
    "        prev_layer = layers.Reshape((last_conv_shape[1],last_conv_shape[2], last_conv_shape[3]))(prev_layer)\n",
    "        channels = last_conv_shape[3]\n",
    "\n",
    "        channels = channels*2\n",
    "        prev_layer = self.create_conv_block(prev_layer, channels,h_activation, 1, norm=True)\n",
    "\n",
    "\n",
    "        for i in range(self.num_downsampling):\n",
    "            channels //= 2 \n",
    "            prev_layer = self.create_upsampling_conv_block(prev_layer, channels, h_activation)\n",
    "        return prev_layer\n",
    "\n",
    "       \n",
    "    def create_conv_block(self, prev_layer, channels, activation, kernel_size=3, padding='same', norm=False):\n",
    "      prev_layer = layers.Conv2D(channels, kernel_size, padding=padding, use_bias=True)(prev_layer)\n",
    "      if(norm):\n",
    "          prev_layer = layers.BatchNormalization()(prev_layer)\n",
    "      prev_layer = layers.LeakyReLU(alpha=0.1)(prev_layer) \n",
    "      return prev_layer\n",
    "\n",
    "    def create_downsampling_conv_block(self, prev_layer, channels, activation, kernel_size=4):\n",
    "        prev_layer = layers.ZeroPadding2D()(prev_layer)\n",
    "        prev_layer = layers.Conv2D(channels, kernel_size, strides=2, use_bias=False)(prev_layer)\n",
    "        prev_layer = layers.BatchNormalization()(prev_layer)\n",
    "        prev_layer = layers.LeakyReLU(alpha=0.1)(prev_layer) \n",
    "        return prev_layer\n",
    "\n",
    "    def create_upsampling_conv_block(self, prev_layer, channels, activation, kernel_size = 3):\n",
    "        prev_layer = layers.Conv2DTranspose(channels, kernel_size,strides=2, padding=\"same\")(prev_layer)\n",
    "        prev_layer = layers.BatchNormalization()(prev_layer)\n",
    "        # prev_layer = layers.Activation(activation)(prev_layer) \n",
    "        prev_layer = layers.LeakyReLU(alpha=0.1)(prev_layer) \n",
    "        return prev_layer\n",
    "\n",
    "    def sampling(self, args):\n",
    "      mu, log_var = args\n",
    "      batch_size = K.shape(mu)[0]\n",
    "      dim = K.int_shape(mu)[1]\n",
    "      epsilon = K.random_normal(shape=(batch_size, dim), mean=0., stddev=1.0)\n",
    "      return K.exp(0.5 * log_var) * epsilon + mu\n",
    "    \n",
    "\n",
    "def vae_loss(vae_input,vae_ouput,mu,log_var,kl_coefficient, input_count):\n",
    "  #Reconstruction loss\n",
    "  vae_input = vae_input[0] if(type(vae_input) is list) else vae_input\n",
    "  x = keras.layers.Reshape((input_count,))(vae_input)\n",
    "  y = keras.layers.Reshape((input_count,))(vae_ouput)\n",
    "  reconstruction_loss = keras.losses.mean_squared_error(x, y) * input_count\n",
    "  # reconstruction_loss = keras.losses.mean_squared_error(vae_input,vae_ouput) * input_count\n",
    "\n",
    "  #Regularization loss\n",
    "  kl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)\n",
    "\n",
    "  #Combined loss\n",
    "  return reconstruction_loss + kl_coefficient*kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_dimension = 32\n",
    "internal_dense_layers = [2048, 1024]\n",
    "vae, vae_encoder, vae_decoder = CCVAE().build_ccvae(\n",
    "   image_shape, \n",
    "   internal_dense_layers, \n",
    "   latent_space_dimension,\n",
    "   'LeakyReLU',\n",
    "   one_hot_label_len)\n",
    "\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(vae, show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compilation\n",
    "kl_coefficient=1\n",
    "\n",
    "#Information needed to compute the loss function\n",
    "vae_input=vae.input\n",
    "vae_output=vae.output\n",
    "mu=vae.get_layer('mu').output\n",
    "log_var=vae.get_layer('log_var').output\n",
    "\n",
    "vae.add_loss(vae_loss(vae_input, vae_output, mu, log_var, kl_coefficient, num_pixels))\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(clipnorm=0.001)) \n",
    "\n",
    "\n",
    "loss_metrics=[]\n",
    "val_loss_metrics=[]\n",
    "fid_frequency_metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 32\n",
    "image_plot_frequency = 8 # how often input-output comparation should be ploted\n",
    "fid_frequency = 8 # how often fid should be calculated\n",
    "\n",
    "def batch_eleboration(model, generator, validation=False):\n",
    "   n = 0\n",
    "   loss_sum = 0\n",
    "   for _ in range(len(generator)):\n",
    "      batch_x, batch_y = next(generator)\n",
    "      if(validation):\n",
    "         loss = model.test_on_batch([batch_x, batch_y], batch_x)\n",
    "      else:\n",
    "         loss = model.train_on_batch([batch_x, batch_y], batch_x)\n",
    "      n += len(batch_x)\n",
    "      loss_sum += (loss*len(batch_x))\n",
    "   return loss_sum / n\n",
    "\n",
    "\n",
    "for e in range(1, epoch_count+1):\n",
    "   start_time = time.time()\n",
    "\n",
    "   loss = batch_eleboration(vae, train_provider)\n",
    "   loss_metrics.append(loss)\n",
    "\n",
    "   val_loss = batch_eleboration(vae, val_provider, validation=True)\n",
    "   val_loss_metrics.append(val_loss)\n",
    "   \n",
    "   end_time = time.time()\n",
    "   print('Epoch: {0} exec_time={1:.1f}s  loss={2:.3f} val_loss={3:.3f}'.format(e,end_time - start_time, loss, val_loss))\n",
    "\n",
    "   if(e%image_plot_frequency == 0):\n",
    "      plot_model_input_and_output(val_provider, vae) \n",
    "   if(is_fid_active and e%fid_frequency == 0):\n",
    "      image_generator = ConditionalImageGeneratorDecoder(vae_decoder, labels_provider(all_one_hot_labels, BATCH_SIZE))\n",
    "      fid_frequency_metrics.append(compute_fid(train_provider, image_generator, image_shape))\n",
    "\n",
    "plot_model_input_and_output(val_provider, vae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation\n",
    "\n",
    "For each real label generate one fake img: stddev 4.5\n",
    "\n",
    "Another solution that generate random colors and  and article types have 25-60 standard deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_from_array(loss_metrics,val_loss_metrics)\n",
    "\n",
    "if(is_fid_active):\n",
    "   plot_fid(fid_frequency_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_generated_article_types(model, one_hot_len, rows=1, cols=5):\n",
    "  for i in range(one_hot_len):\n",
    "    one_hot = np.zeros(one_hot_len, dtype=float)\n",
    "    one_hot[i] = 1\n",
    "    one_hots = [one_hot] * cols\n",
    "    decoderGen = ConditionalImageGeneratorDecoder(model, label_provider(one_hots))\n",
    "    iterator = iter(decoderGen)\n",
    "\n",
    "    generated_images=[]\n",
    "    for _ in range(rows):\n",
    "        generated_images.append(next(iterator))      \n",
    "\n",
    "    plot_generated_images(generated_images,rows,cols)\n",
    "\n",
    "def plot_model_generated_colorfull_article_types(model, num_classes, one_hot_len, rows=1):\n",
    "  num_colors = one_hot_len - num_classes\n",
    "  for clas in range(num_classes):\n",
    "    one_hots = []\n",
    "    for color  in range(num_classes, one_hot_len):\n",
    "        one_hot = np.zeros(one_hot_len, dtype=float)\n",
    "        one_hot[clas] = 1\n",
    "        one_hot[color] = 1\n",
    "        one_hots.append(one_hot)\n",
    "    decoderGen = ConditionalImageGeneratorDecoder(model, label_provider(one_hots))\n",
    "\n",
    "    for _ in range(rows):\n",
    "      plot_generated_images([next(iter(decoderGen))],1,num_colors)\n",
    "\n",
    "if(with_color_label):\n",
    "  plot_model_generated_colorfull_article_types(vae_decoder, len(CLASSES), one_hot_label_len, rows=2)\n",
    "else:\n",
    "  plot_model_generated_article_types(vae_decoder, one_hot_label_len, rows=1, cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative adversarial network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
